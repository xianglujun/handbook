# 多字段类型

- 多字段特性
  - 厂商名字实现精确匹配
- 使用不同analyzer
  - 不同语言
  - pinyin字段搜索
  - 还支持为搜索和索引指定不提供的analyzer



## Exact Values v.s Full Text

- Exact Values v.s Full Text
  - Exact Values: 包括数字、日期、具体一个字符串
    - 对应ES中的keyword, 不会对该类型的数据进行分词处理
  - Full Text , 全文本，非结构化的文本数据
    - EX中的text



## 自定义分词

- 当ES自带的分词器无法满足时，可以自定义分词器，通过不同的组件实现：
  - Character Filter
  - Tokenizer
  - Token Filter



### Character Filter

- 在Tokenizer之前对文本进行处理，例如：
  - 增加、删除、替换字符
- 会影响Tokenizer 的position和offset 信息
- 配置多个Character Filter
- ES 自带Character Filters
  - HTML strip - 去除html标签
  - Mapping - 字符串替换
  - Pattern replace - 正则匹配替换



### Tokenizer

- 将原始文本按照一定的规则，切分为词(term or token)
- ES 内置的tokenizer
  - wihtespace
  - standard
  - uax_url_email
  - pattern
  - keywork
  - path hierachy
- 可以用Java开发插件，实现自定义Tokenizer



### Token Filters

- 将Tokenizer 输出的单词(term)，执行 增加、修改、删除
- 自带的Token Filters
  - Lowercase
  - stop
  - synonym(近义词)



### 实例

```http
#
POST _analyze
{
  "tokenizer": "keyword",
  "char_filter": ["html_strip"],
  "text":"<p>hello world</p>"
}

# 使用 char filter 进行替换, 将 - 转换为 _
POST _analyze
{
  "tokenizer": "standard",
  "char_filter": [
    {
      "type": "mapping",
      "mappings": ["- => _"]
    }],
    "text": "123-345, Hello-world, 400-400400-01"
}

# char filter 替换表情符号
POST _analyze
{
  "tokenizer": "standard",
  "char_filter": [
    {
      "type" : "mapping",
      "mappings": [":) => happy", ":( => sad"]
    }
    ],
  "text": ["I feel :)", "Feeling :( "]
}

# 正则表达式
GET _analyze
{
  "tokenizer": "standard",
  "char_filter": [
    {
      "type": "pattern_replace",
      "pattern": "http://(.*)",
      "replacement": "$1"
    }
    ],
    "text": ["http://www.baidu.com"]
}

# 按照路径切分
POST _analyze
{
  "tokenizer": "path_hierarchy",
  "text": ["/ab/c/c/d/sas/d"]
}

# whitespace 与 stop
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["stop"],
  "text": ["The abc are characters."]
}

GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["stop", "lowercase"],
  "text": ["The abc are characters."]
}
```

