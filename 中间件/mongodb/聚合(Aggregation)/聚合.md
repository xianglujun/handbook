# 聚合

聚合用于处理多个文档数据，并输出计算结果。在mongodb中，聚合可以完成以下事情:

- 按照指定值对文档数据分组

- 对分组数据进行处理并返回单个结果

- 随着时间变化分析数据

在mongodb中，支持三种方式对聚合的实现，

- 聚合管道(Aggregation Pipeline)

- Map-Reduce

- 单目标聚合操作(Single Operation)

## 1. 聚合管道(Aggregation Pipeline)

一个管道中可以包含多个操作节点，每个阶段具有如下特点：

- 每个阶段为一个操作，每个操作是对输入文档文档进行处理。每个阶段中可以包含了filter, group等操作

- 当一个管道中包含了多个阶段时，上一个阶段的输出为下一个阶段的输入

- 一个聚合管道能够对分组数据返回一个单一的结果值。例如总数，平均值等

> 聚合操作并不会改变文档中的数据，除非聚合操作中包含了`$merge`或者`$set`等阶段。

### 1.1 实例展示

下面就以官方的例子展示聚合的基础用法，这里以订单数据为例子，输入数据:

```shell
db.orders.insertMany( [
   { _id: 0, productName: "Steel beam", status: "new", quantity: 10 },
   { _id: 1, productName: "Steel beam", status: "urgent", quantity: 20 },
   { _id: 2, productName: "Steel beam", status: "urgent", quantity: 30 },
   { _id: 3, productName: "Iron rod", status: "new", quantity: 15 },
   { _id: 4, productName: "Iron rod", status: "urgent", quantity: 50 },
   { _id: 5, productName: "Iron rod", status: "urgent", quantity: 10 }
] )
```

我们以管道的方式实现按照产品名称分组，并计算每个产品的数量。

```shell
db.orders.aggregate( [
   { $match: { status: "urgent" } },
   { $group: { _id: "$productName", sumQuantity: { $sum: "$quantity" } } }
] )
```

在这个操作中包含了两个阶段，

- `$match`
  
  - 该阶段从文档数据中过滤状态为`urgent`的文档
  
  - 并将符合条件的文档输出到`$group`阶段中

- `$group`分组数据
  
  - 按照`productName`进行分组
  
  - 将分组的数据`quantity`数据相加，并保存到`sumQuantity`字段中

则对应的输出结果为:

```json
 [
    { _id: 'Steel beam', sumQuantity: 50 },
    { _id: 'Iron rod', sumQuantity: 60 }
  ]
```

### 1.2 zipcode实例

zipcode展示了聚合的另一种使用方法，数据文件可以下载进行使用。[城市数据](./zipcode.josn.md)

#### 1.2.1 数据结构

在以上导入的数据中，每条数据都包含了以下的字段：

```json
{
  "_id": "10280",
  "city": "NEW YORK",
  "state": "NY",
  "pop": 5574,
  "loc": [
    -74.016323,
    40.710537
  ]
}
```

- `_id`字段记录了城市的编码

- `city`字段记录了城市的名称

- `state`记录了州的缩写

- `pop`记录了这座城市的人口

- `loc`记录了城市的坐标，记录了经度和维度

#### 1.2.2 返回人口超过1千万的城市

则具体的查询方式为

```shell
db.zipcodes.aggregate( [
   { $group: { _id: "$state", totalPop: { $sum: "$pop" } } },
   { $match: { totalPop: { $gte: 10*1000*1000 } } }
] )
```

在以上的查询中包含了两个stage,

- `$group`
  
  - 首先按照state对文档数据进行分组
  
  - 然后对分组文档数据中的pop字段使用`$sum`求和，并将求和数据保存在totalPop字段中
  
  - 将生成的文档数据输出到`$match`阶段中

- `$match`
  
  - 该阶段主要对文档进行过滤，只返回总人口totalPop超过一千万的城市

在`$group`阶段中生成的文档输入如下：

```json
{
  "_id" : "AK",
  "totalPop" : 550043
}
```

在以上的操作中，就相当于执行SQL

```sql
select state, sum(pop) as totalPop 
from zipcodes
group by state
having totalPop >= (10 * 1000 * 1000)
```

在执行上面的聚合操作后，则对应的输出结果为:

```json
[
    { _id: 'PA', totalPop: 11881643 },
    { _id: 'IL', totalPop: 11427576 },
    { _id: 'FL', totalPop: 12686644 },
    { _id: 'OH', totalPop: 10846517 },
    { _id: 'TX', totalPop: 16984601 },
    { _id: 'NY', totalPop: 17990402 },
    { _id: 'CA', totalPop: 29754890 }
  ]
```

#### 1.2.3 返回城市的平均人口

这里用于统计州的平均人口数量，这里就需要统计每个州下的城市数据，以及城市的人口数据，则对应的聚合为:

```shell
db.zipcodes.aggregate( [
   { $group: { _id: { state: "$state", city: "$city" }, pop: { $sum: "$pop" } } },
   { $group: { _id: "$_id.state", avgCityPop: { $avg: "$pop" } } }
] )
```

在以上操作中包含了两个阶段，都是与`$group`来实现，

- `$group`
  
  - 第一个group操作根据state和city两个字段分组，、
  
  - 然后根据分组情况统计人口的总数，通过`$sum`来实现人口的总数统计
  
  > 此时我们看到的数据结构如下：
  > 
  > ```json
  > {
  >   "_id" : {
  >     "state" : "CO",
  >     "city" : "EDGEWATER"
  >   },
  >   "pop" : 13
  > ```

- 第二个`$group`
  
  - 第二个group接收到第一个group的输入文档数据，再次按照state进行分组
  
  - 并通过`$avg`的操作对`pop`属性求平均值

> 在聚合管道中，当我们需要取文档中的字段值时，则根据`$`表达式进行获取

#### 1.2.4 获取每个州人口最多和最少的城市已经人口数量

```shell
db.zipcodes.aggregate( [
   { $group:
      {
        _id: { state: "$state", city: "$city" },
        pop: { $sum: "$pop" }
      }
   },
   { $sort: { pop: 1 } },
   { $group:
      {
        _id : "$_id.state",
        biggestCity:  { $last: "$_id.city" },
        biggestPop:   { $last: "$pop" },
        smallestCity: { $first: "$_id.city" },
        smallestPop:  { $first: "$pop" }
      }
   },
  { $project:
    { _id: 0,
      state: "$_id",
      biggestCity:  { name: "$biggestCity",  pop: "$biggestPop" },
      smallestCity: { name: "$smallestCity", pop: "$smallestPop" }
    }
  }
] )
```

在以上的操作总，包含了四个阶段的操作:

- `$group`
  
  - 按照state和city进行分组
  
  - 并对分组内的人口pop进行求和
  
  > 该步骤完成后，对应的数据结构为:
  > 
  > ```json
  > {
  >   "_id" : {
  >     "state" : "CO",
  >     "city" : "EDGEWATER"
  >   },
  >   "pop" : 13154
  > }
  > ```

- `$sort`
  
  - 是对上一步的分组求和的结果进行排序
  
  - `pop`指定了排序的规则
    
    - 1 - 正序
    
    - -1 - 倒序

- `$group`
  
  - 该group中按照state字段进行分组
  
  - `$last`是获取分组中的最后一个文档数据
  
  - `$first`获取文档中的第一个元素
  
  > 该步骤在完成后，则对应的数据结构为
  > 
  > ```json
  > {
  >   "_id" : "WA",
  >   "biggestCity" : "SEATTLE",
  >   "biggestPop" : 520096,
  >   "smallestCity" : "BENGE",
  >   "smallestPop" : 2
  > }
  > ```

- `$project`该阶段是作为可选阶段，可以对返回的文档数据进行重写
  
  - 在这个阶段中，对返回的数据格式进行了改写。则这个阶段返回的结果数据为
  
  > 此时则对应的返回结果为:
  > 
  > ```json
  > {
  >       biggestCity: { name: 'PORTLAND', pop: 518543 },
  >       smallestCity: { name: 'LYONS', pop: 0 },
  >       state: 'OR'
  >     }
  > ```

### 1.3 聚合管道限制

在聚合管道中，是存在一些限制的，主要包括一下两点：

#### 1.3.1 结果大小限制

mongodb中对于单个文档的大小是有限制的，目前打个文档大小为`16m`. 如果单个文档超过了`BSON的16m`限制，聚合管道将会报错。但是这个限制只是针对返回的文档大小，在聚合管道执行的过程中，是完全可能超过这个限制的。在`db.collection.aggregation()`方法中，默认返回的是一个`cursor`对象。

#### 1.3.2 内存大小限制

聚合管道也是有内存大小限制的，默认每隔管道使用的最大内存为`100m`, 当超过这个限制的时候，聚合管道将会报错。当管道使用的内存超过内存限制时，可以通过`allowDiskUse`参数将管道输入写入到磁盘临时文件中。

> `$search`操作不受100m内存的限制，因为该操作是以单独的进程执行

以下操作在指定`allowDiskUse=true`的时候，会将数据写入到磁盘中:

- `$bucket`

- `$bucketAuto`

- `$group`

- `$sort`只有排序操作不支持索引时

- `$sortByCount`

> 聚合管理以流式方式获取文档，处理文档，然后返回文档，但是有的阶段并不会马上返回文档，而是会等到所有文档都处理完成后一起输出，这时数据是存储在内存中，这样的话，内存中的文档大小是完全可能超过100m的。

如果一个`$sort`的操作的文档大小超过100m的时候，建议与`$limit`阶段一起使用。

#### 1.3.4 聚合管道与集合分片

聚合管道支持在分片集合上进行操作，不过这种操作具有一定的前提。主要包含两种情况

- 如果聚合分片在`$match`操作上明确指定了`shard key`信息，并且不包含`$out`, `$lookup`阶段，则整个聚合管道在指定的分片上执行

- 如果聚合管道在多个分片上执行，则需要在`mongos`上进行数据的合并，主要包含一下两种情况：
  
  - 如果聚合管道中包含了`$out`和`$lookup`阶段操作，则合并操作必须在主分片(primary shard)上进行
  
  - 如果聚合管道中包含了`$sort`和`$group`阶段并且`allowDiskUse=true`，此时数据合并将会在随机分片上进行。

在聚合管道在多分片上执行的时候，本身会设计到聚合管道的优化，会将管道换分为两部分，然后尽可能多的在多分片上并行执行管道中的阶段，已达到优化效果。

### 1.4 管道优化

在mongodb中，本身存在着对管道的优化，通过重塑管道，以提升管道的执行性能。为了能够看到mongodb对于管道的优化，我们可以在`db.collection.aggregation()`方法中加入`explain`参数，以查看mongodb对于管道的优化信息。

#### 1.4.1 Projection优化

在聚合中，mongodb会分析使用的字段是否只是文档字段的一部分，当整个管道只是使用部分字段时，mongodb将不会获取文档的整个字段列表，而是根据需要获取字段，减少管道中的数据传输量。

#### 1.4.2 管道顺序优化

##### 1.4.2.1 (`$project/$unset/$addFields/$set`) + `$match`

当在一个管道中，如果`project/unset/addFields/set`后面跟了`$match`的阶段，此时mongodb会将`$match`中未参加计算的字段创建一个新的`$match`阶段到`projection`阶段的前面。

如果在管道中包含了多个`projection`的阶段和`$match`阶段，这时会将所有`$match`中未参与计算的字段形成新的`$match`, 并防止到所有的`projection`前面。

例如有一下管道操作：

```json
{ $addFields: {
    maxTime: { $max: "$times" },
    minTime: { $min: "$times" }
} },
{ $project: {
    _id: 1, name: 1, times: 1, maxTime: 1, minTime: 1,
    avgTime: { $avg: ["$maxTime", "$minTime"] }
} },
{ $match: {
    name: "Joe Schmoe",
    maxTime: { $lt: 20 },
    minTime: { $gt: 5 },
    avgTime: { $gt: 7 }
} }
```

在以上的查询中，mongodb将把`$match`操作进行拆分，然后进可能的将新创建的`$match`放到单独的过滤条件中，并穿插到不同的`projection`语法前面。则可能最终的管道语句为:

```shell
{ $match: { name: "Joe Schmoe" } },
{ $addFields: {
    maxTime: { $max: "$times" },
    minTime: { $min: "$times" }
} },
{ $match: { maxTime: { $lt: 20 }, minTime: { $gt: 5 } } },
{ $project: {
    _id: 1, name: 1, times: 1, maxTime: 1, minTime: 1,
    avgTime: { $avg: ["$maxTime", "$minTime"] }
} },
{ $match: { avgTime: { $gt: 7 } } }
```

- 在`{ avgTime: { $gt: 7 } }`的`$match`中，字段`avtTime`依赖的是`$project`操作的返回的平均时间，因此该`$match`操作无法移动，只能放到最后面位置

- `maxTime`和`minTime`依赖了`$addFields`操作产生的新字段，但是并不依赖于`$project`操作，因此mongodb生成新的`$match`操作，并放到了`$project`操作的前面。

- `$match`过滤条件`{ name: "Joe Schmoe" }`并不依赖于`$addFields`和`$project`的任何操作，因此该过滤条件放到了所有阶段的前面。

> 在以上操作中，过滤条件`{ name: "Joe Schmoe" }`放到管道的最前面的好处就在于，在过滤数据的时候，我们能够使用索引过滤文档数据，而不是扫描全部的文档数据。(这建立在name字段创建了索引)

##### 1.4.2.2 `$match` + `$sort`

`$sort` 和`$match` 的配合使用，主要优化点在于优先过滤数据，然后再执行排序。

例如定义管道如下:

```json
{ $sort: { age : -1 } },
{ $match: { status: 'A' } }
```

则在执行的时候，则对应的管道为:

```json
{ $match: { status: 'A' } },
{ $sort: { age : -1 } }
```

这样的顺序交换主要减少查询文档的数量，减少排序的文档数量，对结果并没有任何影响。

##### 1.4.2.3 `$redact` + `$match`

当聚合管道中`$redact`后紧跟`$match`操作，mongodb可能会将`$match`中的一部分创建新的`$match`阶段放置到`$redact`前面，如果新增`$match`是在管道的开始，就可以使用索引过滤文档，以减少进入到管道中的文档数据。

例如以下管道定义:

```json
{ $redact: { $cond: { if: { $eq: [ "$level", 5 ] }, then: "$$PRUNE", else: "$$DESCEND" } } },
{ $match: { year: 2014, category: { $ne: "Z" } } }
```

则优化器在优化完成后，则管道变更为:

```json
{ $match: { year: 2014 } },
{ $redact: { $cond: { if: { $eq: [ "$level", 5 ] }, then: "$$PRUNE", else: "$$DESCEND" } } },
{ $match: { year: 2014, category: { $ne: "Z" } } }
```

##### 1.4.2.4 `$project/$unset` + `$skip`

从mongodb 3.2版本开始，当`$project`或者`$unset`后紧跟`$skip`阶段时，此时`$kip`将会被移动到`$project/$unset`阶段前面。

例如有以下管道定义:

```json
{ $sort: { age : -1 } },
{ $project: { status: 1, name: 1 } },
{ $skip: 5 }
```

则优化后的管道定义为:

```json
{ $sort: { age : -1 } },
{ $skip: 5 },
{ $project: { status: 1, name: 1 } }
```

#### 1.4.3 管道合并优化(Coalescence)

在聚合管道中，可能会将部分的阶段合并到上一个阶段

##### 1.4.3.1 `$sort`+`$limit`

当管道中包含了`$sort`和`$limit`操作时，此时可能会将`$limit`合并到`$sort`操作中，但是这个合并是有前提条件的:

- 只有在`$sort`和`$limit`操作中没有包含其他可能改变文档数量的操作时(例如`$unwind/$group`)，才能进行合并操作

例如有以下管道定义:

```json
{ $sort : { age : -1 } },
{ $project : { age : 1, status : 1, name : 1 } },
{ $limit: 5 }
```

则优化之后的管道变为:

```json
{
    "$sort" : {
       "sortKey" : {
          "age" : -1
       },
       "limit" : NumberLong(5)
    }
},
{ "$project" : {
         "age" : 1,
         "status" : 1,
         "name" : 1
  }
}
```

> 当`$sort`和`$limit`操作中包含了`$skip`阶段时，将`$limit`合并到`$sort`阶段时，需要加上`$skip`的数值。

这样的合并能够减少通过管道的文档数据和保存在内存中的文档数量，这就相当于`从1千万个数据中返回最小的五个数，此时内存中只需要维护5个数字即可`

##### 1.4.3.2 `$limit` + `$limit`

当`$limit`阶段后面紧跟`$limit`操作时，此时两个阶段可以合并，并且取两个`$limit`操作中的最小值。

例如有以下管道定义:

```json
{ $limit: 100 },
{ $limit: 10 }
```

则优化之后的管道为:

```json
{ $limit: 10 }
```

##### 1.4.3.3 `$skip` + `$skip`

当`$skip`后面紧跟`$skip`阶段时，两个阶段合并为一个`$skip`阶段，并且取两个`$skip`数值的和。

例如有一下管道定义：

```json
{ $skip: 5 },
{ $skip: 2 }
```

则优化后的管道为:

```json
{ $skip: 7 }
```

##### 1.4.3.4 `$match` + `$match`

当`$match`阶段后紧跟`$match`操作时，可以合并两个阶段为一个`$match`并通过`$and`合并两个过滤条件。

例如有一下管道定义：

```json
{ $match: { year: 2014 } },
{ $match: { status: "A" } }
```

则优化后的管道定义为:

```json
{ $match: { $and: [ { "year" : 2014 }, { "status" : "A" } ] } }
```

##### 1.4.3.5 `$lookup` + `$unwind`

当`$lookup`紧跟`$unwind`阶段时，并且`$unwind`使用了`$lookup`中的`as`字段信息，mongodb将合并两个阶段，防止创建大量中间文档数据。

例如有如下管道定义:

```json
{
  $lookup: {
    from: "otherCollection",
    as: "resultingArray",
    localField: "x",
    foreignField: "y"
  }
},
{ $unwind: "$resultingArray"}
```

则优化之后的管道定义为:

```json
{
  $lookup: {
    from: "otherCollection",
    as: "resultingArray",
    localField: "x",
    foreignField: "y",
    unwinding: { preserveNullAndEmptyArrays: false }
  }
}
```

#### 1.4.4 索引

在管道中使用索引能够大大的优化管道的性能，因为当查询使用索引的时候，可以大大减少管道处理的文档数量，也能够通过索引返回查询需要的文档。

例如，假如一个管道包含了`$match`、`$sort`、`$group`阶段时，能够从索引获取以下好处:

- `$match`数据能够快速查询关联的文档数据

- `$sort`索引能够在该阶段返回有序的文档数据

- 在具有索引字段上执行`$group`操作时，能够使用`$sort`排序快速的执行分组，并且返回所需要的字段值。

在管道中有多个阶段都可以从索引上获取性能上的提升：

- `$match`当该阶段处于管道开始位置时，能够通过索引快速过滤文档数据

- `$sort`能够从索引中获取排序性能上的提升，但是文档数据不能被`$project/$unwind/$group`操作处理

- 如果能够满足一下条件，`$group`能够通过索引快速获取到每个分组的第一个元素：
  
  - 在`$group`之前，分组文档被`$sort`排序过
  
  - 在`$group`字段上包含索引，并且排序与索引字段顺序保持一致时
  
  - 在`$group`中只有`$first`一个归集操作

- `$geonear`该阶段始终能够使用索引，该阶段必须为管道第一个阶段并且包含了`geospatial索引`


