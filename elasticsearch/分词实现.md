# 分词器

## Analysis 与 Analyzer

- Analysis - 文本分析是把全文本转换成为一系列单词(term / token)的过程，也叫分词
- Analysis 是通过 Analyzer来实现的
  - 可使用ES内置的分析器或者按需定制化分析器
- 当ES 匹配Query语句时候，也需要相同的分析器对查询语句进行分析

### Analyzer 组成部分

- 分词器是专门处理分词的组件，Analyzer由三部分组成
  - Charater Filter - 针对原始文本的处理
  - Tokenizer - 按照规则切分为单词
  - Token Filter - 将切分的单词进行加工， 小写，删除 stopwords, 增加同义词



### ES 内置分词器

- Standar Analyzer - 默认分词器，按词切分，小写处理
- Simple Analyzer - 按照非字母切分(符号被过滤), 小写处理
- Stop Analyzer - 小写处理，停用词过滤
- Whitespace Analyzer - 按空格切分，不转小写
- Keyword Analyzer - 部分词，直接将输入当作输出
- Patter Analyzer - 正则表达式，默认\W(非字符分割)
- Language - 提供了30多重常见语言的分词器
- Customer Anaylyzer 自定义分词器



## API

### _analyze API

- 直接指定Analyzer 进行测试
- 指定索引的字段进行测试
- 自定义分词器进行测试



## Standard Analyzer

- 默认分词器
- 按词切分
- 将词转换为小写

```http
GET /_analyze
{
  "analyzer": "standard",
  "text": "I have a blog web site, and 40+ posts to view"
}
```

```json
{
  "tokens" : [
    {
      "token" : "i",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "have",
      "start_offset" : 2,
      "end_offset" : 6,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "a",
      "start_offset" : 7,
      "end_offset" : 8,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "blog",
      "start_offset" : 9,
      "end_offset" : 13,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "web",
      "start_offset" : 14,
      "end_offset" : 17,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "site",
      "start_offset" : 18,
      "end_offset" : 22,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "and",
      "start_offset" : 24,
      "end_offset" : 27,
      "type" : "<ALPHANUM>",
      "position" : 6
    },
    {
      "token" : "40",
      "start_offset" : 28,
      "end_offset" : 30,
      "type" : "<NUM>",
      "position" : 7
    },
    {
      "token" : "posts",
      "start_offset" : 32,
      "end_offset" : 37,
      "type" : "<ALPHANUM>",
      "position" : 8
    },
    {
      "token" : "to",
      "start_offset" : 38,
      "end_offset" : 40,
      "type" : "<ALPHANUM>",
      "position" : 9
    },
    {
      "token" : "view",
      "start_offset" : 41,
      "end_offset" : 45,
      "type" : "<ALPHANUM>",
      "position" : 10
    }
  ]
}

```



## Simple Analyzer

- 按照非字母切分，非字母的都被去除
- 小写处理

![image-20210424104513710](.\image-20210424104513710.png)

```http
GET /_analyze
{
  "analyzer": "simple",
  "text": "I have a blog web site, and 40+ posts to view"
}
```

```json
{
  "tokens" : [
    {
      "token" : "i",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "have",
      "start_offset" : 2,
      "end_offset" : 6,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "a",
      "start_offset" : 7,
      "end_offset" : 8,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "blog",
      "start_offset" : 9,
      "end_offset" : 13,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "web",
      "start_offset" : 14,
      "end_offset" : 17,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "site",
      "start_offset" : 18,
      "end_offset" : 22,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "and",
      "start_offset" : 24,
      "end_offset" : 27,
      "type" : "word",
      "position" : 6
    },
    {
      "token" : "posts",
      "start_offset" : 32,
      "end_offset" : 37,
      "type" : "word",
      "position" : 7
    },
    {
      "token" : "to",
      "start_offset" : 38,
      "end_offset" : 40,
      "type" : "word",
      "position" : 8
    },
    {
      "token" : "view",
      "start_offset" : 41,
      "end_offset" : 45,
      "type" : "word",
      "position" : 9
    }
  ]
}

```



## Whitespace Analyzer

- 按照空格切分单词

```http
GET /_analyze
{
  "analyzer": "whitespace",
  "text": "I have a blog web site, and 40+ posts to view"
}
```

```json
{
  "tokens" : [
    {
      "token" : "I",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "have",
      "start_offset" : 2,
      "end_offset" : 6,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "a",
      "start_offset" : 7,
      "end_offset" : 8,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "blog",
      "start_offset" : 9,
      "end_offset" : 13,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "web",
      "start_offset" : 14,
      "end_offset" : 17,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "site,",
      "start_offset" : 18,
      "end_offset" : 23,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "and",
      "start_offset" : 24,
      "end_offset" : 27,
      "type" : "word",
      "position" : 6
    },
    {
      "token" : "40+",
      "start_offset" : 28,
      "end_offset" : 31,
      "type" : "word",
      "position" : 7
    },
    {
      "token" : "posts",
      "start_offset" : 32,
      "end_offset" : 37,
      "type" : "word",
      "position" : 8
    },
    {
      "token" : "to",
      "start_offset" : 38,
      "end_offset" : 40,
      "type" : "word",
      "position" : 9
    },
    {
      "token" : "view",
      "start_offset" : 41,
      "end_offset" : 45,
      "type" : "word",
      "position" : 10
    }
  ]
}

```



## Stop Analyzer

- 相比Simple Analyzer, 多了Stop Filter
  - 会把`the, a, is `等修饰词语去除
- 将单词转换为小写

```http
GET /_analyze
{
  "analyzer": "stop",
  "text": "I have a blog web site, and 40+ posts to view"
}
```

```json
{
  "tokens" : [
    {
      "token" : "i",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "have",
      "start_offset" : 2,
      "end_offset" : 6,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "blog",
      "start_offset" : 9,
      "end_offset" : 13,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "web",
      "start_offset" : 14,
      "end_offset" : 17,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "site",
      "start_offset" : 18,
      "end_offset" : 22,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "posts",
      "start_offset" : 32,
      "end_offset" : 37,
      "type" : "word",
      "position" : 7
    },
    {
      "token" : "view",
      "start_offset" : 41,
      "end_offset" : 45,
      "type" : "word",
      "position" : 9
    }
  ]
}

```



## Keyword Analyzer

- 部分词，直接将输入当作输出

```http
GET /_analyze
{
  "analyzer": "keyword",
  "text": "I have a blog web site, and 40+ posts to view"
}
```

```json
{
  "tokens" : [
    {
      "token" : "I have a blog web site, and 40+ posts to view",
      "start_offset" : 0,
      "end_offset" : 45,
      "type" : "word",
      "position" : 0
    }
  ]
}

```

## Pattern Analyzer

- 通过正则表达式进行分词
- 默认是\W+ 实现分词
- 将分词转换为小写

```http
GET /_analyze
{
  "analyzer": "pattern",
  "text": "I have a blog web site, and 40+ posts to view"
}
```

```json
{
  "tokens" : [
    {
      "token" : "i",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "have",
      "start_offset" : 2,
      "end_offset" : 6,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "a",
      "start_offset" : 7,
      "end_offset" : 8,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "blog",
      "start_offset" : 9,
      "end_offset" : 13,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "web",
      "start_offset" : 14,
      "end_offset" : 17,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "site",
      "start_offset" : 18,
      "end_offset" : 22,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "and",
      "start_offset" : 24,
      "end_offset" : 27,
      "type" : "word",
      "position" : 6
    },
    {
      "token" : "40",
      "start_offset" : 28,
      "end_offset" : 30,
      "type" : "word",
      "position" : 7
    },
    {
      "token" : "posts",
      "start_offset" : 32,
      "end_offset" : 37,
      "type" : "word",
      "position" : 8
    },
    {
      "token" : "to",
      "start_offset" : 38,
      "end_offset" : 40,
      "type" : "word",
      "position" : 9
    },
    {
      "token" : "view",
      "start_offset" : 41,
      "end_offset" : 45,
      "type" : "word",
      "position" : 10
    }
  ]
}

```



## Language 分词

- 包含了stop过滤规则

```http
GET /_analyze
{
  "analyzer": "english",
  "text": "I have a blog web site, and 40+ posts to view"
}
```

```json
{
  "tokens" : [
    {
      "token" : "i",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "have",
      "start_offset" : 2,
      "end_offset" : 6,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "blog",
      "start_offset" : 9,
      "end_offset" : 13,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "web",
      "start_offset" : 14,
      "end_offset" : 17,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "site",
      "start_offset" : 18,
      "end_offset" : 22,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "40",
      "start_offset" : 28,
      "end_offset" : 30,
      "type" : "<NUM>",
      "position" : 7
    },
    {
      "token" : "post",
      "start_offset" : 32,
      "end_offset" : 37,
      "type" : "<ALPHANUM>",
      "position" : 8
    },
    {
      "token" : "view",
      "start_offset" : 41,
      "end_offset" : 45,
      "type" : "<ALPHANUM>",
      "position" : 10
    }
  ]
}

```



## 中文分词

- 中文句子，切分为一个一个次
- 英文中，单词有自然的空格作为分割
- 一句中文，在不同的上下文，有不同的理解



### ICU Analyzer

![image-20210424112443517](.\image-20210424112443517.png)

- 需要安装plugin
  - elasticsearch-plugin install analysis-icu
- 提供了Unicode的支持，更好的支持亚洲语言

```http
GET /_analyze
{
  "analyzer": "icu_analyzer",
  "text": "你说的没错"
}
```

```json
{
  "tokens" : [
    {
      "token" : "你",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "<IDEOGRAPHIC>",
      "position" : 0
    },
    {
      "token" : "说的",
      "start_offset" : 1,
      "end_offset" : 3,
      "type" : "<IDEOGRAPHIC>",
      "position" : 1
    },
    {
      "token" : "没错",
      "start_offset" : 3,
      "end_offset" : 5,
      "type" : "<IDEOGRAPHIC>",
      "position" : 2
    }
  ]
}

```



## 分词推荐

- IK
  - 支持自定义词库，支持热更新分词字典
- THULAC
  - 清华大学自然语言处理和社会人文计算实验室的一套中文分词器

